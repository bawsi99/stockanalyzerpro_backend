# Agent Migration Guide: backend/gemini ‚Üí backend/llm

This guide explains how to migrate agents from the old `backend/gemini` system to the new `backend/llm` system.

## Overview

The new `backend/llm` system provides:
- ‚úÖ **Provider-agnostic** design (Gemini, OpenAI, Claude, etc.)
- ‚úÖ **Model-specific configuration** per agent
- ‚úÖ **Simple, clean API** calls without over-engineering
- ‚úÖ **Easy to test** and maintain

## Migration Pattern (Risk Agent Example)

The risk analysis agent was successfully migrated as a proof of concept. Here's the step-by-step pattern:

### Step 1: Add Agent Configuration

Add your agent to `backend/llm/config/llm_assignments.yaml`:

```yaml
agents:
  # Your Agent Name
  your_agent:
    provider: "gemini"
    model: "gemini-2.5-flash"  # or gemini-2.5-pro for complex reasoning
    enable_code_execution: true  # if needed for calculations
    max_retries: 3
    timeout: 90  # adjust based on expected processing time
```

### Step 2: Update Agent Implementation

**Before (using GeminiClient):**
```python
class YourAgent:
    def __init__(self):
        from gemini.gemini_client import GeminiClient
        self.gemini_client = GeminiClient(api_key=api_key, agent_name="your_agent")
    
    async def analyze(self, data):
        # Uses GeminiClient's context_engineer and prompt_manager
        context = self.gemini_client.context_engineer.structure_context(...)
        prompt = self.gemini_client.prompt_manager.format_prompt("template", context=context)
        response = await self.gemini_client.core.call_llm_with_code_execution(prompt)
```

**After (using backend/llm):**
```python
class YourAgent:
    def __init__(self):
        # Import with proper path handling
        import sys
        import os
        parent_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        if parent_path not in sys.path:
            sys.path.insert(0, parent_path)
        from backend.llm import get_llm_client
        
        self.llm_client = get_llm_client("your_agent")  # Uses config from YAML
        # Keep existing prompt and context logic
        self.prompt_template = self._load_prompt_template()
    
    async def analyze(self, data):
        # Build context using your own logic (no context_engineer dependency)
        context = self._build_context(data)
        # Build prompt using your own logic (no prompt_manager dependency)  
        prompt = self._build_prompt(context)
        # Call LLM using provider-agnostic interface
        response = await self.llm_client.generate(prompt, enable_code_execution=True)
```

### Step 3: Handle Module Import Issues

**Problem**: Agents that create global instances may fail with `ModuleNotFoundError: No module named 'backend'`

**Solution**: Use lazy initialization pattern:

```python
# At bottom of agent file - OLD WAY (causes import errors)
# your_agent = YourAgent()  # ‚ùå Don't do this

# NEW WAY - Lazy initialization
_your_agent_instance = None

def get_your_agent():
    """Get or create the global agent instance."""
    global _your_agent_instance
    if _your_agent_instance is None:
        _your_agent_instance = YourAgent()
    return _your_agent_instance

# For backwards compatibility
your_agent = None  # Will be set on first access
```

Update `__init__.py`:
```python
from .your_agent import YourAgent, get_your_agent

def _get_your_agent():
    return get_your_agent()

your_agent = _get_your_agent  # Function to get agent

__all__ = ['YourAgent', 'get_your_agent', 'your_agent']
```

### Step 4: Update Service References

Update any services that import the agent:

**Before:**
```python
from agents.your_package.your_agent import your_agent
result = await your_agent.analyze(data)
```

**After:**
```python
from agents.your_package.your_agent import get_your_agent
agent = get_your_agent()
result = await agent.analyze(data)
```

### Step 5: Test the Migration

Create a test script (see `backend/agents/risk_analysis/test_migration.py` for example):

```python
async def test_your_agent_migration():
    # Test 1: Import the migrated agent
    from agents.your_package.your_agent import YourAgent
    
    # Test 2: Initialize the agent  
    agent = YourAgent()
    print(f"Provider: {agent.llm_client.get_provider_info()}")
    print(f"Config: {agent.llm_client.get_config()}")
    
    # Test 3: Test prompt building (without LLM call)
    prompt = agent._build_prompt(mock_data)
    assert prompt, "Prompt building failed"
    
    print("‚úÖ Migration successful!")
```

## Agent Types & Migration Complexity

### Easy Migration (Like Risk Agent)
**Characteristics:**
- ‚ùå Does NOT use `context_engineer.structure_context()`
- ‚ùå Does NOT use `prompt_manager.format_prompt()`  
- ‚úÖ Has own prompt loading and context building logic
- ‚úÖ Only uses `GeminiClient.core` for LLM calls

**Examples:** Risk Agent, Volume Agents

**Migration:** Simple - just replace `GeminiClient.core` with `backend/llm`

### Medium Migration (Like Final Decision Agent)
**Characteristics:**
- ‚úÖ Uses GeminiClient methods for context building
- ‚úÖ Uses `prompt_manager.format_prompt()`
- ‚ö†Ô∏è Moderately coupled to GeminiClient helper methods

**Examples:** Final Decision Agent

**Migration:** Move GeminiClient helper methods to agent-specific code

### Complex Migration (Like Indicators Agent)
**Characteristics:**
- ‚úÖ Heavily uses `context_engineer.structure_context()`
- ‚úÖ Uses `AnalysisType` enums and complex structuring logic  
- ‚úÖ Tightly coupled to GeminiClient context engineering

**Examples:** Indicators Agent, Some Volume Agents

**Migration:** Move entire context engineering logic to agent-specific code

## Benefits After Migration

### For Agents:
- üéØ **Agent-specific configuration**: Each agent can use different models/providers
- üß™ **Easy testing**: No complex GeminiClient dependencies  
- üîß **Simple maintenance**: Clear separation of concerns
- üìù **Clean code**: Agent owns its prompts and context logic

### For System:
- üîÑ **Provider flexibility**: Switch between Gemini/OpenAI/Claude per agent
- ‚ö° **Better performance**: Reduced overhead and complexity
- üõ†Ô∏è **Easier debugging**: Clear request/response flow
- üìä **Better monitoring**: Provider-specific metrics and logging

## Next Steps

1. **Start with easy agents** (like Risk Agent)
2. **Test thoroughly** with actual API calls
3. **Apply same pattern** to medium complexity agents
4. **Handle complex agents last** (may need more planning)
5. **Update documentation** as you learn from each migration

## Migration Checklist

- [ ] Add agent configuration to `llm_assignments.yaml`
- [ ] Update agent implementation to use `backend/llm`
- [ ] Fix module import paths  
- [ ] Handle global instance creation (lazy initialization)
- [ ] Update service references
- [ ] Create migration test script
- [ ] Test with actual API keys
- [ ] Update documentation
- [ ] Monitor for regressions in production

## Troubleshooting

### Import Errors
**Error:** `ModuleNotFoundError: No module named 'backend'`

**Solution:** Add proper path handling in agent `__init__`:
```python
import sys
import os
parent_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
if parent_path not in sys.path:
    sys.path.insert(0, parent_path)
```

### Configuration Errors  
**Error:** `ValueError: Unsupported provider: xyz`

**Solution:** Check `llm_assignments.yaml` has correct provider name (`gemini`, `openai`, `claude`)

### API Key Errors
**Error:** `ValueError: Gemini API key is required`

**Solution:** Ensure environment variables are set (`GEMINI_API_KEY1-5` or `GEMINI_API_KEY`)

---

## Success Story: Risk Agent Migration

The risk analysis agent migration was completed successfully with:

**Results:**
- ‚úÖ **Agent import: SUCCESS**  
- ‚úÖ **Agent initialization: SUCCESS**
- ‚úÖ **LLM client setup: SUCCESS**
- ‚úÖ **Prompt template loading: SUCCESS**  
- ‚úÖ **Prompt building: SUCCESS**

**Configuration Used:**
```yaml
risk_agent:
  provider: "gemini"
  model: "gemini-2.5-flash"
  enable_code_execution: true
  max_retries: 3
  timeout: 90
```

**Key Changes:**
- Replaced `GeminiClient()` ‚Üí `get_llm_client("risk_agent")`
- Replaced `self.gemini_client.core.call_llm_with_code_execution()` ‚Üí `self.llm_client.generate()`
- Added proper path handling for imports
- Used lazy initialization for global instance
- Updated service references to use getter function

This migration pattern can now be applied to other agents with similar characteristics.